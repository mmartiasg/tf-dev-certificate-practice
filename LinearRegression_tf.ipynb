{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement linear regression on tensorflow with gradient tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = \"datasets/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATASETS+os.sep+\"winequality-red.csv\", \"r\") as file:\n",
    "    raw_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = raw_data.split(\"\\n\")[1:]\n",
    "columns = raw_data.split(\"\\n\")[0].split(\",\")\n",
    "\n",
    "N_ROWS = len(raw_dataset)\n",
    "N_FEATURES = len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROWS: \", N_ROWS)\n",
    "print(\"Features: \", N_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.zeros((N_ROWS, N_FEATURES))\n",
    "\n",
    "i = 0\n",
    "for row in raw_dataset:\n",
    "    j = 0\n",
    "    for feature in row.split(\",\"):\n",
    "        dataset[i][j] = float(feature)\n",
    "        j+=1\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[:, 7:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset into features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SEQ_FEATURES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.constant(dataset[:, :N_SEQ_FEATURES], dtype=\"float32\")\n",
    "y_train = tf.constant(dataset[:, -1], dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset into train and validation\n",
    "\n",
    "into 70/30\n",
    "\n",
    "- Shuffle dataset\n",
    "- Select N for Train and N for Validation base on the proportio 70/20/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = np.random.shuffle(dataset)\n",
    "N_train = int(dataset.shape[0]*0.7)+1\n",
    "N_val = int(dataset.shape[0]*0.2)+1\n",
    "N_test = dataset.shape[0]-N_train-N_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train rows: \", N_train)\n",
    "print(\"Validation rows: \", N_val)\n",
    "print(\"Test rows: \", N_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data \n",
    "Into \n",
    "- Train\n",
    "- Val\n",
    "- Test \n",
    "\n",
    "And X for features and y for the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = dataset[:N_train, :N_SEQ_FEATURES]\n",
    "train_y = dataset[:N_train, -1]\n",
    "\n",
    "val_X = dataset[:N_val, :N_SEQ_FEATURES]\n",
    "val_y = dataset[:N_val, -1]\n",
    "\n",
    "test_X = dataset[:N_test, :N_SEQ_FEATURES]\n",
    "test_y = dataset[:N_test, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First model Baseline AVG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_prediction = train_y.mean()\n",
    "\n",
    "print(\"Pred for baseline: \", baseline_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((val_y - baseline_prediction)**2).sum()/val_y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([4,4,4,4])/np.array([2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.max(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.min(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_max = train_X.max(axis=0)\n",
    "train_min = train_X.min(axis=0)\n",
    "\n",
    "Q_factor = 100\n",
    "\n",
    "train_X -= train_min\n",
    "train_X /= (train_max-train_min) * Q_factor\n",
    "\n",
    "val_X -= train_min\n",
    "val_X /= (train_max-train_min) * Q_factor\n",
    "\n",
    "test_X -= train_min\n",
    "test_X /= (train_max-train_min) * Q_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First model Regresion\n",
    "\n",
    "Y = W.X + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_N_DIMS = train_X.shape[1]\n",
    "B_N_DIMS = train_X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(np.random.ranf((1, W_N_DIMS)), dtype='float32')\n",
    "b = tf.Variable(np.random.ranf((B_N_DIMS, 1)), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_tensor = tf.constant(train_X, dtype='float32')\n",
    "train_y_tensor = tf.constant(train_y, dtype='float32')\n",
    "\n",
    "val_x_tensor = tf.constant(val_X, dtype='float32')\n",
    "val_y_tensor = tf.constant(val_y, dtype='float32')\n",
    "\n",
    "test_x_tensor = tf.constant(test_X, dtype='float32')\n",
    "test_y_tensor = tf.constant(test_y, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W (1, DIM) * X (N, DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try random values first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.matmul(W, tf.transpose(train_x_tensor)) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.reduce_mean(tf.pow(tf.subtract(y, train_y_tensor), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try with gradien tape to fix the weights\n",
    "\n",
    "Problems I ran into\n",
    "\n",
    "- I got none because I have declared W and b as constants!!! derivative is 0!\n",
    "- Im getting nan an inf!!?\n",
    "    - Standarizing values too small result in some of them being too big I had to adjust that (max-min) by Q=100\n",
    "- Mac tensorflow cant work with float16!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y = tf.matmul(train_x_tensor, tf.transpose(W)) + b\n",
    "    loss = tf.math.reduce_mean(tf.pow(tf.subtract(y, train_y_tensor), 2))\n",
    "\n",
    "gradient_loss_w_b = tape.gradient(loss, [W, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W [1, 11]\n",
    "train_x_tensor[1120, 11]\n",
    "\n",
    "W*train_x_tensor [1120, 1]\n",
    "\n",
    "b [1120, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init params\n",
    "W = tf.Variable(np.random.ranf((1, W_N_DIMS)), dtype='float32')\n",
    "b = tf.Variable(np.random.ranf((B_N_DIMS, 1)), dtype='float32')\n",
    "\n",
    "#init epsilon\n",
    "epsilon = tf.constant(0.01, dtype='float32')\n",
    "\n",
    "for epoc in tqdm(range(5000)):\n",
    "    # Feed-forward pass\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = tf.matmul(train_x_tensor, tf.transpose(W)) + b\n",
    "        loss = tf.reduce_mean(tf.square(train_y_tensor-y))\n",
    "\n",
    "    if epoc%1000==0:\n",
    "        print(\"TRAIN Loss: \", loss)\n",
    "        print(f\"First sample training set prediction {y[0]} - real value {train_y_tensor[0]}\")\n",
    "\n",
    "    #backward - pass\n",
    "    w_grad, b_grad = tape.gradient(loss, [W, b])\n",
    "\n",
    "    W.assign_sub(epsilon*w_grad)\n",
    "    b.assign_sub(epsilon*b_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch approach is faster to converge\n",
    "\n",
    "as it is able to adjust weights faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init params\n",
    "BATCH_SIZE = 32\n",
    "W = tf.Variable(np.random.ranf((1, W_N_DIMS)), dtype='float32')\n",
    "b = tf.Variable(np.random.ranf((BATCH_SIZE, 1)), dtype='float32')\n",
    "MAX_EPOCHS = 5000\n",
    "\n",
    "#init epsilon\n",
    "epsilon = tf.constant(0.01, dtype='float32')\n",
    "\n",
    "for epoc in tqdm(range(MAX_EPOCHS)):\n",
    "    \n",
    "    # Feed-forward pass\n",
    "    for batch in range(0, B_N_DIMS, BATCH_SIZE):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = tf.matmul(train_x_tensor[batch:batch+BATCH_SIZE], tf.transpose(W)) + b\n",
    "            loss = tf.reduce_mean(tf.square(train_y_tensor[batch:batch+BATCH_SIZE]-y))\n",
    "        \n",
    "        #backward - pass\n",
    "        w_grad, b_grad = tape.gradient(loss, [W, b])\n",
    "\n",
    "        W.assign_sub(epsilon*w_grad)\n",
    "        b.assign_sub(epsilon*b_grad)\n",
    "\n",
    "    if epoc%1000==0:\n",
    "        print(f\"advance: {np.round(epoc/MAX_EPOCHS, 2)*100}%\")\n",
    "        val_loss = 0\n",
    "        for batch in range(0, val_x_tensor.shape[0], BATCH_SIZE):\n",
    "            y_val = tf.matmul(val_x_tensor[batch:batch+BATCH_SIZE], tf.transpose(W)) + b\n",
    "            val_loss += tf.reduce_mean(tf.square(val_y_tensor[batch:batch+BATCH_SIZE]-y_val))\n",
    "        print(\"TRAIN Loss: \", loss)\n",
    "        print(\"VAL loss: \", val_loss/(int(val_x_tensor.shape[0]/BATCH_SIZE)+1))\n",
    "        print(f\"First sample training set prediction {y[0]} - real value {train_y_tensor[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch with momemtum\n",
    "\n",
    "# With RMS to avoid getting stuck in a local minima around [0.557] en val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init params\n",
    "BATCH_SIZE = 32\n",
    "MAX_EPOCHS = 5000\n",
    "\n",
    "W = tf.Variable(np.random.ranf((1, W_N_DIMS)), dtype='float32')\n",
    "b = tf.Variable(np.random.ranf((BATCH_SIZE, 1)), dtype='float32')\n",
    "\n",
    "#RMS\n",
    "velocity_w = tf.Variable(np.zeros((1, W_N_DIMS)), dtype='float32')\n",
    "velocity_b = tf.Variable(np.zeros((BATCH_SIZE, 1)), dtype='float32')\n",
    "past_velocity_w = tf.Variable(np.zeros((1, W_N_DIMS)), dtype='float32')\n",
    "past_velocity_b = tf.Variable(np.zeros((BATCH_SIZE, 1)), dtype='float32')\n",
    "momentum = tf.constant(0.1, dtype='float32')\n",
    "\n",
    "#init epsilon\n",
    "epsilon = tf.constant(0.01, dtype='float32')\n",
    "\n",
    "for epoc in tqdm(range(MAX_EPOCHS)):\n",
    "    # Feed-forward pass\n",
    "    for batch in range(0, B_N_DIMS, BATCH_SIZE):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # print(f\"Range: {batch} , {batch+BATCH_SIZE}\")\n",
    "            y = tf.matmul(train_x_tensor[batch:batch+BATCH_SIZE], tf.transpose(W)) + b\n",
    "            loss = tf.reduce_mean(tf.square(train_y_tensor-y))\n",
    " \n",
    "        #backward - pass\n",
    "        w_grad, b_grad = tape.gradient(loss, [W, b])\n",
    "\n",
    "        velocity_w.assign(past_velocity_w*momentum-epsilon*w_grad)\n",
    "        velocity_b.assign(past_velocity_b*momentum-epsilon*b_grad)\n",
    "\n",
    "        W.assign_add(velocity_w*momentum-epsilon*w_grad)\n",
    "        b.assign_add(velocity_b*momentum-epsilon*b_grad)\n",
    "\n",
    "        past_velocity_w.assign(velocity_w)\n",
    "        past_velocity_b.assign(velocity_b)\n",
    "\n",
    "    if epoc%1000==0:\n",
    "        print(f\"advance: {np.round(epoc/MAX_EPOCHS, 2)}%\")\n",
    "        val_loss = 0\n",
    "        for batch in range(0, val_x_tensor.shape[0], BATCH_SIZE):\n",
    "            y_val = tf.matmul(val_x_tensor[batch:batch+BATCH_SIZE], tf.transpose(W)) + b\n",
    "            val_loss += tf.reduce_mean(tf.square(val_y_tensor[batch:batch+BATCH_SIZE]-y_val))\n",
    "        print(\"TRAIN Loss: \", loss)\n",
    "        print(\"VAL loss: \", val_loss/(int(val_x_tensor.shape[0]/BATCH_SIZE)+1))\n",
    "        print(f\"First sample training set prediction {y[0]} - real value {train_y_tensor[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
